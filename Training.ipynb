{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 18027868259578936892\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3211018240\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5357067110326638216\n",
      "physical_device_desc: \"device: 0, name: Quadro M2200, pci bus id: 0000:01:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "import os\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import hamming_loss\n",
    "import shutil\n",
    "from IPython.display import display\n",
    "import importlib\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "posters_dir = os.path.join('.', 'posters')\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "gpu_devices = [device for device in device_lib.list_local_devices() if device ]\n",
    "\n",
    "if len(gpu_devices) == 0:\n",
    "    raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape original: (26585, 9)\n",
      "Shape filtered: (10813, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>tmdbId</th>\n",
       "      <th>release_year</th>\n",
       "      <th>poster_url</th>\n",
       "      <th>language</th>\n",
       "      <th>local_poster_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>[Adventure, Animation, Children, Comedy, Fantasy]</td>\n",
       "      <td>114709</td>\n",
       "      <td>862</td>\n",
       "      <td>1995</td>\n",
       "      <td>https://image.tmdb.org/t/p/w300/rhIRbceoE9lR4v...</td>\n",
       "      <td>en</td>\n",
       "      <td>1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>[Adventure, Children, Fantasy]</td>\n",
       "      <td>113497</td>\n",
       "      <td>8844</td>\n",
       "      <td>1995</td>\n",
       "      <td>https://image.tmdb.org/t/p/w300/vgpXmVaVyUL7GG...</td>\n",
       "      <td>en</td>\n",
       "      <td>2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>[Comedy, Romance]</td>\n",
       "      <td>113228</td>\n",
       "      <td>15602</td>\n",
       "      <td>1995</td>\n",
       "      <td>https://image.tmdb.org/t/p/w300/6ksm1sjKMFLbO7...</td>\n",
       "      <td>en</td>\n",
       "      <td>3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>[Comedy, Drama, Romance]</td>\n",
       "      <td>114885</td>\n",
       "      <td>31357</td>\n",
       "      <td>1995</td>\n",
       "      <td>https://image.tmdb.org/t/p/w300/16XOMpEaLWkrcP...</td>\n",
       "      <td>en</td>\n",
       "      <td>4.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>[Comedy]</td>\n",
       "      <td>113041</td>\n",
       "      <td>11862</td>\n",
       "      <td>1995</td>\n",
       "      <td>https://image.tmdb.org/t/p/w300/e64sOI48hQXyru...</td>\n",
       "      <td>en</td>\n",
       "      <td>5.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                              genres  imdbId  tmdbId  \\\n",
       "0  [Adventure, Animation, Children, Comedy, Fantasy]  114709     862   \n",
       "1                     [Adventure, Children, Fantasy]  113497    8844   \n",
       "2                                  [Comedy, Romance]  113228   15602   \n",
       "3                           [Comedy, Drama, Romance]  114885   31357   \n",
       "4                                           [Comedy]  113041   11862   \n",
       "\n",
       "   release_year                                         poster_url language  \\\n",
       "0          1995  https://image.tmdb.org/t/p/w300/rhIRbceoE9lR4v...       en   \n",
       "1          1995  https://image.tmdb.org/t/p/w300/vgpXmVaVyUL7GG...       en   \n",
       "2          1995  https://image.tmdb.org/t/p/w300/6ksm1sjKMFLbO7...       en   \n",
       "3          1995  https://image.tmdb.org/t/p/w300/16XOMpEaLWkrcP...       en   \n",
       "4          1995  https://image.tmdb.org/t/p/w300/e64sOI48hQXyru...       en   \n",
       "\n",
       "  local_poster_file  \n",
       "0             1.jpg  \n",
       "1             2.jpg  \n",
       "2             3.jpg  \n",
       "3             4.jpg  \n",
       "4             5.jpg  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import helpers\n",
    "importlib.reload(helpers)\n",
    "\n",
    "def split_and_sort(x):\n",
    "    l = x.split('|')\n",
    "    l.sort()\n",
    "    return l\n",
    "\n",
    "def read_movies_data():\n",
    "    return pd.read_csv('extended_movie_data_with_local_files.csv', sep=';')\n",
    "\n",
    "def filter_and_enrich_movies_data(movies):\n",
    "    movies = helpers.filter_movies(movies)\n",
    "    movies['release_year'] = pd.to_numeric(movies.release_year)\n",
    "    movies.dropna(inplace=True)\n",
    "    movies['genres'] = movies['genres'].apply(split_and_sort)\n",
    "    return movies\n",
    "\n",
    "movies = read_movies_data()\n",
    "print('Shape original: {}'.format(movies.shape))\n",
    "movies = filter_and_enrich_movies_data(movies)\n",
    "print('Shape filtered: {}'.format(movies.shape))\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>count</th>\n",
       "      <th>distribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Drama</td>\n",
       "      <td>4784</td>\n",
       "      <td>0.222109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comedy</td>\n",
       "      <td>3482</td>\n",
       "      <td>0.161660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Thriller</td>\n",
       "      <td>2170</td>\n",
       "      <td>0.100747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Documentary</td>\n",
       "      <td>1695</td>\n",
       "      <td>0.078694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Romance</td>\n",
       "      <td>1467</td>\n",
       "      <td>0.068109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Action</td>\n",
       "      <td>1417</td>\n",
       "      <td>0.065788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Horror</td>\n",
       "      <td>1151</td>\n",
       "      <td>0.053438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Crime</td>\n",
       "      <td>1141</td>\n",
       "      <td>0.052974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adventure</td>\n",
       "      <td>823</td>\n",
       "      <td>0.038210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>661</td>\n",
       "      <td>0.030689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mystery</td>\n",
       "      <td>568</td>\n",
       "      <td>0.026371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Children</td>\n",
       "      <td>557</td>\n",
       "      <td>0.025860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fantasy</td>\n",
       "      <td>535</td>\n",
       "      <td>0.024839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Animation</td>\n",
       "      <td>436</td>\n",
       "      <td>0.020242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>War</td>\n",
       "      <td>281</td>\n",
       "      <td>0.013046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Musical</td>\n",
       "      <td>260</td>\n",
       "      <td>0.012071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Western</td>\n",
       "      <td>88</td>\n",
       "      <td>0.004086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Film-Noir</td>\n",
       "      <td>23</td>\n",
       "      <td>0.001068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          genre  count  distribution\n",
       "6         Drama   4784      0.222109\n",
       "3        Comedy   3482      0.161660\n",
       "9      Thriller   2170      0.100747\n",
       "15  Documentary   1695      0.078694\n",
       "5       Romance   1467      0.068109\n",
       "7        Action   1417      0.065788\n",
       "10       Horror   1151      0.053438\n",
       "8         Crime   1141      0.052974\n",
       "0     Adventure    823      0.038210\n",
       "12       Sci-Fi    661      0.030689\n",
       "11      Mystery    568      0.026371\n",
       "2      Children    557      0.025860\n",
       "4       Fantasy    535      0.024839\n",
       "1     Animation    436      0.020242\n",
       "13          War    281      0.013046\n",
       "14      Musical    260      0.012071\n",
       "16      Western     88      0.004086\n",
       "17    Film-Noir     23      0.001068"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def calc_genre_counts(genres_per_movie):\n",
    "    flat_list = [genre for movie_genres in genres_per_movie for genre in movie_genres]\n",
    "    genre_counts = Counter(flat_list)\n",
    "    df = pd.DataFrame(list(genre_counts.items()), columns=['genre', 'count'])\n",
    "    df = df.sort_values(['count'], ascending=False)\n",
    "    df['distribution'] = df['count']/df['count'].sum()\n",
    "    return df\n",
    "\n",
    "genre_counts = calc_genre_counts(movies.genres)\n",
    "genre_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres to drop: {'Sci-Fi', 'Musical', 'Western', 'War', 'Adventure', 'Mystery', 'Film-Noir', 'Animation', 'Children', 'Fantasy'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10502, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_rare_genres(df):\n",
    "    movies = df.copy(deep=True)\n",
    "    to_remove = set(genre_counts[genre_counts['count'] < 1000]['genre'])\n",
    "    print('Genres to drop: {}'.format(to_remove))\n",
    "    movies['relevant_genres'] = movies['genres'].apply(lambda x: [genre for genre in x if genre not in to_remove])\n",
    "    cleaned_genre_movies = movies[movies.relevant_genres.map(len) > 0]\n",
    "    return cleaned_genre_movies\n",
    "\n",
    "drop_rare_genres(movies).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres to drop: {'Sci-Fi', 'Musical', 'Western', 'War', 'Adventure', 'Mystery', 'Film-Noir', 'Animation', 'Children', 'Fantasy'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>count</th>\n",
       "      <th>distribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drama</td>\n",
       "      <td>4784</td>\n",
       "      <td>0.276420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comedy</td>\n",
       "      <td>3482</td>\n",
       "      <td>0.201190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thriller</td>\n",
       "      <td>2170</td>\n",
       "      <td>0.125383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Documentary</td>\n",
       "      <td>1695</td>\n",
       "      <td>0.097937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Romance</td>\n",
       "      <td>1467</td>\n",
       "      <td>0.084763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Action</td>\n",
       "      <td>1417</td>\n",
       "      <td>0.081874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Horror</td>\n",
       "      <td>1151</td>\n",
       "      <td>0.066505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crime</td>\n",
       "      <td>1141</td>\n",
       "      <td>0.065927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         genre  count  distribution\n",
       "2        Drama   4784      0.276420\n",
       "0       Comedy   3482      0.201190\n",
       "5     Thriller   2170      0.125383\n",
       "7  Documentary   1695      0.097937\n",
       "1      Romance   1467      0.084763\n",
       "3       Action   1417      0.081874\n",
       "6       Horror   1151      0.066505\n",
       "4        Crime   1141      0.065927"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = drop_rare_genres(movies)\n",
    "calc_genre_counts(movies.relevant_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of classes: 8\n",
      "\n",
      "Example: \n",
      "[['Comedy', 'Drama', 'Romance']]\n",
      "[[0 1 0 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "label_binarizer = MultiLabelBinarizer()\n",
    "label_binarizer.fit(movies['relevant_genres'])\n",
    "\n",
    "print('Count of classes: {}'.format(len(label_binarizer.classes_)))\n",
    "\n",
    "def print_label_bin_example():\n",
    "    print('\\nExample: ')\n",
    "    example = [movies.iloc[2]['relevant_genres']]\n",
    "    print(example)\n",
    "    print(label_binarizer.transform(example))\n",
    "\n",
    "print_label_bin_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre count average 1.6479718148924014\n",
      "Genre count variance 0.6053658160024004\n"
     ]
    }
   ],
   "source": [
    "def print_dataset_statistics(df):\n",
    "    lengths_of_genres = df['relevant_genres'].apply(lambda x: len(x))\n",
    "    genres_count_average = np.mean(lengths_of_genres)\n",
    "    print('Genre count average {}'.format(genres_count_average))\n",
    "    genres_count_variance = np.var(lengths_of_genres)\n",
    "    print('Genre count variance {}'.format(genres_count_variance))\n",
    "    \n",
    "print_dataset_statistics(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation = train_test_split(movies, test_size=0.1, random_state=1)\n",
    "X_train, X_test = train_test_split(X_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "\n",
    "def calc_performance_metric(y_true, y_predicted):\n",
    "    return roc_auc_score(y_true, y_predicted, average='macro')\n",
    "\n",
    "def calc_optimal_thresholds(y_true, y_predicted):\n",
    "    optimal_thresholds = []\n",
    "    \n",
    "    for i in range(y_true.shape[1]):\n",
    "        column_true = y_true[:, i]\n",
    "        column_predicted = y_predicted[:, i]\n",
    "        fpr, tpr, thresholds = roc_curve(column_true, column_predicted)\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        optimal_thresholds.append(optimal_threshold)\n",
    "        \n",
    "    return optimal_thresholds\n",
    "\n",
    "def test_optimal_thresholds_calc():\n",
    "    optimal_thresholds = calc_optimal_thresholds(np.array([[1], [0]]), np.array([[0.61], [0.59]]))\n",
    "    assert len(optimal_thresholds) == 1, 'Len: {}'.format(len(optimal_thresholds))\n",
    "    assert 0.59 < optimal_thresholds[0] <= 0.61 , str(optimal_thresholds)\n",
    "    \n",
    "test_optimal_thresholds_calc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_accuracy(y_true, y_predicted_values, thresholds):\n",
    "    y_predicted = np.zeros(y_true.shape)\n",
    "    for column in range(y_true.shape[1]):\n",
    "        y_predicted[:, column] = (y_predicted_values[:, column] > thresholds[column]).astype(int)\n",
    "       \n",
    "    return np.mean([accuracy_score(y_true[:, col], y_predicted[:, col]) for col in range(y_true.shape[1])])\n",
    "\n",
    "assert calc_mean_accuracy(np.array([[1, 1], [0, 1]]), np.array([[1, 1], [1, 1]]), [.5, .5]) == 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_model_performance(labels, predictions):\n",
    "    performance_metric = calc_performance_metric(labels, predictions)\n",
    "    optimal_thresholds = calc_optimal_thresholds(labels, predictions)\n",
    "    mean_accuracy = calc_mean_accuracy(labels, predictions, optimal_thresholds)\n",
    "    return performance_metric, optimal_thresholds, mean_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224\n",
    "#img_width, img_height = 299, 299\n",
    "\n",
    "def batch_generator(dataframe, distortion=False, batch_size=20):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         horizontal_flip=False\n",
    "    )   \n",
    "    \n",
    "    row_generator = dataframe.sample(frac=1).iterrows()\n",
    "    \n",
    "    for batch_count in range(int(dataframe.shape[0]/batch_size)):\n",
    "        \n",
    "        batch_img = []\n",
    "        batch_labels = []\n",
    "        batch_img_files = []\n",
    "        \n",
    "        #start_time = time.time()\n",
    "        \n",
    "        for img_count in range(batch_size):\n",
    "            _, row = next(row_generator)\n",
    "\n",
    "            img_path = os.path.join(posters_dir, row['local_poster_file'])\n",
    "            img = load_img(img_path, target_size=(img_width, img_height))\n",
    "            img = img_to_array(img)\n",
    "            if hasattr(img, 'close'):\n",
    "                img.close()\n",
    "            if distortion:\n",
    "                img = train_datagen.random_transform(img)\n",
    "            img = train_datagen.standardize(img)\n",
    "\n",
    "            label = label_binarizer.transform([row['relevant_genres']])[0]\n",
    "\n",
    "            batch_img.append(img)\n",
    "            batch_labels.append(label)\n",
    "            batch_img_files.append(row['local_poster_file'])\n",
    "\n",
    "        yield [np.array(batch_img), np.array(batch_labels), batch_img_files]\n",
    "            \n",
    "    return\n",
    "\n",
    "#print(X_train.shape)\n",
    "#next(batch_generator(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, Input\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.layers import concatenate\n",
    "\n",
    "from bp_mll_keras import bp_mll_loss\n",
    "\n",
    "def create_model():\n",
    "    base_model = applications.VGG16(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))\n",
    "    #base_model = applications.InceptionV3(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))\n",
    "\n",
    "    #print(base_model.input.name)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "\n",
    "    #tags = Input(shape=(1,) , name='tags')\n",
    "    #x = concatenate([x, tags])\n",
    "    #x = Flatten()(x)\n",
    "\n",
    "    predictions = Dense(len(label_binarizer.classes_), activation='sigmoid')(x)\n",
    "    model = Model(inputs=[base_model.input], outputs=predictions)\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "    #model.compile(optimizer='rmsprop', loss=bp_mll_loss)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: 0.5062896768965802, Mean Accuracy: 0.3554947668886774\n"
     ]
    }
   ],
   "source": [
    "def calc_model_predictions(model, data):\n",
    "    inputs, y_true, _ = next(batch_generator(data, batch_size=data.shape[0], distortion=False))\n",
    "    return y_true, model.predict(inputs)\n",
    "\n",
    "def print_model_performance(model, data):\n",
    "    y_true, predictions = calc_model_predictions(model, X_validation)\n",
    "    calc_model_performance(y_true, predictions)\n",
    "    performance_metric, optimal_thresholds, mean_accuracy = calc_model_performance(y_true, predictions)\n",
    "    print(\"Performance: {}, Mean Accuracy: {}\".format(performance_metric, mean_accuracy))\n",
    "    \n",
    "print_model_performance(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:11 Batches 10, Train Loss: 3.124, Performance metric: 0.627, Mean Accuracy: 0.6176470588235294\n",
      "0:00:21 Batches 20, Train Loss: 2.998, Performance metric: 0.684, Mean Accuracy: 0.6529411764705882\n",
      "0:00:31 Batches 30, Train Loss: 2.985, Performance metric: 0.694, Mean Accuracy: 0.6323529411764706\n",
      "0:00:39 Batches 40, Train Loss: 2.991, Performance metric: 0.701, Mean Accuracy: 0.7036764705882353\n",
      "0:00:47 Batches 50, Train Loss: 3.009, Performance metric: 0.700, Mean Accuracy: 0.6279411764705882\n",
      "0:00:55 Batches 60, Train Loss: 2.996, Performance metric: 0.706, Mean Accuracy: 0.6580882352941176\n",
      "0:01:03 Batches 70, Train Loss: 3.016, Performance metric: 0.710, Mean Accuracy: 0.6419117647058823\n",
      "0:01:11 Batches 80, Train Loss: 3.044, Performance metric: 0.715, Mean Accuracy: 0.6845588235294118\n",
      "0:01:19 Batches 90, Train Loss: 3.051, Performance metric: 0.719, Mean Accuracy: 0.6977941176470588\n",
      "0:01:27 Batches 100, Train Loss: 3.048, Performance metric: 0.720, Mean Accuracy: 0.7213235294117648\n",
      "VALIDATION: Performance metric: 0.735, Mean Accuracy: 0.688035204567079\n",
      "0:02:06 Batches 110, Train Loss: 3.034, Performance metric: 0.721, Mean Accuracy: 0.6926470588235294\n",
      "0:02:17 Batches 120, Train Loss: 3.015, Performance metric: 0.721, Mean Accuracy: 0.6786764705882353\n",
      "0:02:26 Batches 130, Train Loss: 3.004, Performance metric: 0.716, Mean Accuracy: 0.6985294117647058\n",
      "0:02:35 Batches 140, Train Loss: 2.991, Performance metric: 0.719, Mean Accuracy: 0.7095588235294117\n",
      "0:02:44 Batches 150, Train Loss: 2.990, Performance metric: 0.727, Mean Accuracy: 0.6852941176470588\n",
      "0:02:52 Batches 160, Train Loss: 2.994, Performance metric: 0.729, Mean Accuracy: 0.6808823529411765\n",
      "0:02:59 Batches 170, Train Loss: 2.983, Performance metric: 0.732, Mean Accuracy: 0.6727941176470589\n",
      "0:03:07 Batches 180, Train Loss: 2.976, Performance metric: 0.727, Mean Accuracy: 0.6926470588235294\n",
      "0:03:15 Batches 190, Train Loss: 2.988, Performance metric: 0.736, Mean Accuracy: 0.7139705882352941\n",
      "0:03:23 Batches 200, Train Loss: 2.987, Performance metric: 0.734, Mean Accuracy: 0.6919117647058823\n",
      "VALIDATION: Performance metric: 0.747, Mean Accuracy: 0.6743577545195053\n",
      "0:03:52 Batches 210, Train Loss: 2.979, Performance metric: 0.738, Mean Accuracy: 0.6911764705882353\n",
      "0:04:00 Batches 220, Train Loss: 2.982, Performance metric: 0.738, Mean Accuracy: 0.7022058823529411\n",
      "0:04:10 Batches 230, Train Loss: 2.976, Performance metric: 0.736, Mean Accuracy: 0.7088235294117646\n",
      "0:04:20 Batches 240, Train Loss: 2.967, Performance metric: 0.740, Mean Accuracy: 0.7139705882352941\n",
      "0:04:29 Batches 250, Train Loss: 2.968, Performance metric: 0.738, Mean Accuracy: 0.7080882352941176\n",
      "0:04:37 Batches 260, Train Loss: 2.966, Performance metric: 0.736, Mean Accuracy: 0.7051470588235293\n",
      "0:04:45 Batches 270, Train Loss: 2.968, Performance metric: 0.739, Mean Accuracy: 0.6933823529411764\n",
      "0:04:53 Batches 280, Train Loss: 2.965, Performance metric: 0.739, Mean Accuracy: 0.6779411764705882\n",
      "0:05:01 Batches 290, Train Loss: 2.958, Performance metric: 0.740, Mean Accuracy: 0.6727941176470589\n",
      "0:05:09 Batches 300, Train Loss: 2.955, Performance metric: 0.736, Mean Accuracy: 0.6625\n",
      "VALIDATION: Performance metric: 0.755, Mean Accuracy: 0.6914843006660324\n",
      "0:05:37 Batches 310, Train Loss: 2.955, Performance metric: 0.745, Mean Accuracy: 0.6823529411764706\n",
      "0:06:01 Batches 320, Train Loss: 2.955, Performance metric: 0.750, Mean Accuracy: 0.6955882352941176\n",
      "0:06:12 Batches 330, Train Loss: 2.953, Performance metric: 0.752, Mean Accuracy: 0.7036764705882352\n",
      "0:06:21 Batches 340, Train Loss: 2.947, Performance metric: 0.752, Mean Accuracy: 0.7117647058823529\n",
      "0:06:30 Batches 350, Train Loss: 2.935, Performance metric: 0.751, Mean Accuracy: 0.7147058823529412\n",
      "0:06:38 Batches 360, Train Loss: 2.938, Performance metric: 0.759, Mean Accuracy: 0.7029411764705883\n",
      "0:06:46 Batches 370, Train Loss: 2.931, Performance metric: 0.754, Mean Accuracy: 0.6948529411764706\n",
      "0:06:57 Batches 380, Train Loss: 2.926, Performance metric: 0.757, Mean Accuracy: 0.7044117647058823\n",
      "0:07:13 Batches 390, Train Loss: 2.924, Performance metric: 0.757, Mean Accuracy: 0.7051470588235293\n",
      "0:07:24 Batches 400, Train Loss: 2.923, Performance metric: 0.758, Mean Accuracy: 0.6941176470588235\n",
      "VALIDATION: Performance metric: 0.759, Mean Accuracy: 0.7059942911512845\n",
      "0:08:13 Batches 410, Train Loss: 2.921, Performance metric: 0.755, Mean Accuracy: 0.6992647058823529\n",
      "0:08:24 Batches 420, Train Loss: 2.920, Performance metric: 0.757, Mean Accuracy: 0.6838235294117647\n",
      "0:08:36 Batches 10, Train Loss: 2.864, Performance metric: 0.758, Mean Accuracy: 0.7147058823529412\n",
      "0:08:45 Batches 20, Train Loss: 2.827, Performance metric: 0.758, Mean Accuracy: 0.7095588235294117\n",
      "0:08:53 Batches 30, Train Loss: 2.804, Performance metric: 0.759, Mean Accuracy: 0.7176470588235293\n",
      "0:09:01 Batches 40, Train Loss: 2.803, Performance metric: 0.757, Mean Accuracy: 0.7125\n",
      "0:09:09 Batches 50, Train Loss: 2.887, Performance metric: 0.763, Mean Accuracy: 0.7169117647058822\n",
      "0:09:32 Batches 60, Train Loss: 2.864, Performance metric: 0.764, Mean Accuracy: 0.7227941176470589\n",
      "0:09:43 Batches 70, Train Loss: 2.867, Performance metric: 0.765, Mean Accuracy: 0.713235294117647\n",
      "0:09:53 Batches 80, Train Loss: 2.876, Performance metric: 0.767, Mean Accuracy: 0.7272058823529413\n",
      "0:10:02 Batches 90, Train Loss: 2.873, Performance metric: 0.763, Mean Accuracy: 0.7066176470588235\n",
      "0:10:10 Batches 100, Train Loss: 2.882, Performance metric: 0.763, Mean Accuracy: 0.700735294117647\n",
      "VALIDATION: Performance metric: 0.761, Mean Accuracy: 0.6976688867745005\n",
      "0:10:39 Batches 110, Train Loss: 2.862, Performance metric: 0.764, Mean Accuracy: 0.7073529411764706\n",
      "0:10:47 Batches 120, Train Loss: 2.854, Performance metric: 0.769, Mean Accuracy: 0.7183823529411765\n",
      "0:10:55 Batches 130, Train Loss: 2.862, Performance metric: 0.768, Mean Accuracy: 0.7169117647058824\n",
      "0:11:03 Batches 140, Train Loss: 2.856, Performance metric: 0.765, Mean Accuracy: 0.6963235294117647\n",
      "0:11:11 Batches 150, Train Loss: 2.843, Performance metric: 0.769, Mean Accuracy: 0.7176470588235295\n",
      "0:11:19 Batches 160, Train Loss: 2.841, Performance metric: 0.770, Mean Accuracy: 0.725735294117647\n",
      "0:11:27 Batches 170, Train Loss: 2.841, Performance metric: 0.768, Mean Accuracy: 0.7080882352941176\n",
      "0:11:35 Batches 180, Train Loss: 2.837, Performance metric: 0.771, Mean Accuracy: 0.7117647058823529\n",
      "0:11:47 Batches 190, Train Loss: 2.831, Performance metric: 0.773, Mean Accuracy: 0.7073529411764705\n",
      "0:11:57 Batches 200, Train Loss: 2.830, Performance metric: 0.771, Mean Accuracy: 0.7058823529411764\n",
      "VALIDATION: Performance metric: 0.765, Mean Accuracy: 0.7095623215984777\n",
      "0:12:30 Batches 210, Train Loss: 2.831, Performance metric: 0.770, Mean Accuracy: 0.7213235294117647\n",
      "0:12:38 Batches 220, Train Loss: 2.837, Performance metric: 0.769, Mean Accuracy: 0.7132352941176471\n",
      "0:12:46 Batches 230, Train Loss: 2.827, Performance metric: 0.766, Mean Accuracy: 0.725735294117647\n",
      "0:12:54 Batches 240, Train Loss: 2.827, Performance metric: 0.772, Mean Accuracy: 0.7301470588235295\n",
      "0:13:02 Batches 250, Train Loss: 2.825, Performance metric: 0.771, Mean Accuracy: 0.724264705882353\n",
      "0:13:10 Batches 260, Train Loss: 2.820, Performance metric: 0.768, Mean Accuracy: 0.7073529411764706\n",
      "0:13:24 Batches 270, Train Loss: 2.814, Performance metric: 0.768, Mean Accuracy: 0.7139705882352941\n",
      "0:13:35 Batches 280, Train Loss: 2.804, Performance metric: 0.768, Mean Accuracy: 0.7294117647058823\n",
      "0:13:45 Batches 290, Train Loss: 2.804, Performance metric: 0.770, Mean Accuracy: 0.723529411764706\n",
      "0:13:54 Batches 300, Train Loss: 2.806, Performance metric: 0.773, Mean Accuracy: 0.7058823529411764\n",
      "VALIDATION: Performance metric: 0.766, Mean Accuracy: 0.7190770694576594\n",
      "0:14:25 Batches 310, Train Loss: 2.795, Performance metric: 0.772, Mean Accuracy: 0.7066176470588235\n",
      "0:14:33 Batches 320, Train Loss: 2.796, Performance metric: 0.776, Mean Accuracy: 0.7147058823529412\n",
      "0:14:41 Batches 330, Train Loss: 2.795, Performance metric: 0.775, Mean Accuracy: 0.7154411764705884\n",
      "0:14:56 Batches 340, Train Loss: 2.792, Performance metric: 0.776, Mean Accuracy: 0.7176470588235294\n",
      "0:15:08 Batches 350, Train Loss: 2.794, Performance metric: 0.776, Mean Accuracy: 0.7220588235294118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:15:18 Batches 360, Train Loss: 2.796, Performance metric: 0.778, Mean Accuracy: 0.7147058823529412\n",
      "0:15:27 Batches 370, Train Loss: 2.802, Performance metric: 0.780, Mean Accuracy: 0.7080882352941177\n",
      "0:15:35 Batches 380, Train Loss: 2.799, Performance metric: 0.779, Mean Accuracy: 0.6970588235294117\n",
      "0:15:43 Batches 390, Train Loss: 2.800, Performance metric: 0.781, Mean Accuracy: 0.7220588235294118\n",
      "0:15:51 Batches 400, Train Loss: 2.796, Performance metric: 0.780, Mean Accuracy: 0.7198529411764707\n",
      "VALIDATION: Performance metric: 0.768, Mean Accuracy: 0.7263320647002853\n",
      "0:16:20 Batches 410, Train Loss: 2.794, Performance metric: 0.784, Mean Accuracy: 0.711764705882353\n",
      "0:16:30 Batches 420, Train Loss: 2.796, Performance metric: 0.784, Mean Accuracy: 0.7205882352941175\n",
      "0:16:55 Batches 10, Train Loss: 3.141, Performance metric: 0.783, Mean Accuracy: 0.7301470588235294\n",
      "0:17:05 Batches 20, Train Loss: 2.916, Performance metric: 0.780, Mean Accuracy: 0.7139705882352941\n",
      "0:17:14 Batches 30, Train Loss: 2.847, Performance metric: 0.778, Mean Accuracy: 0.7507352941176471\n",
      "0:17:23 Batches 40, Train Loss: 2.823, Performance metric: 0.777, Mean Accuracy: 0.7345588235294118\n",
      "0:17:31 Batches 50, Train Loss: 2.795, Performance metric: 0.776, Mean Accuracy: 0.7323529411764705\n",
      "0:17:40 Batches 60, Train Loss: 2.807, Performance metric: 0.774, Mean Accuracy: 0.736764705882353\n",
      "0:17:54 Batches 70, Train Loss: 2.801, Performance metric: 0.778, Mean Accuracy: 0.7213235294117646\n",
      "0:18:10 Batches 80, Train Loss: 2.790, Performance metric: 0.776, Mean Accuracy: 0.7227941176470588\n",
      "0:18:20 Batches 90, Train Loss: 2.745, Performance metric: 0.775, Mean Accuracy: 0.7294117647058823\n",
      "0:18:30 Batches 100, Train Loss: 2.739, Performance metric: 0.778, Mean Accuracy: 0.7132352941176471\n",
      "VALIDATION: Performance metric: 0.771, Mean Accuracy: 0.7326355851569933\n",
      "0:19:01 Batches 110, Train Loss: 2.735, Performance metric: 0.779, Mean Accuracy: 0.7183823529411765\n",
      "0:19:09 Batches 120, Train Loss: 2.730, Performance metric: 0.775, Mean Accuracy: 0.7448529411764706\n",
      "0:19:26 Batches 130, Train Loss: 2.740, Performance metric: 0.783, Mean Accuracy: 0.7404411764705883\n",
      "0:19:37 Batches 140, Train Loss: 2.752, Performance metric: 0.783, Mean Accuracy: 0.7382352941176471\n",
      "0:19:47 Batches 150, Train Loss: 2.729, Performance metric: 0.787, Mean Accuracy: 0.7294117647058824\n",
      "0:19:56 Batches 160, Train Loss: 2.734, Performance metric: 0.782, Mean Accuracy: 0.7051470588235293\n",
      "0:20:05 Batches 170, Train Loss: 2.725, Performance metric: 0.785, Mean Accuracy: 0.7264705882352942\n",
      "0:20:13 Batches 180, Train Loss: 2.731, Performance metric: 0.783, Mean Accuracy: 0.7125\n",
      "0:20:25 Batches 190, Train Loss: 2.736, Performance metric: 0.784, Mean Accuracy: 0.7176470588235293\n",
      "0:20:39 Batches 200, Train Loss: 2.738, Performance metric: 0.782, Mean Accuracy: 0.725735294117647\n",
      "VALIDATION: Performance metric: 0.772, Mean Accuracy: 0.7323977164605138\n",
      "0:21:19 Batches 210, Train Loss: 2.743, Performance metric: 0.786, Mean Accuracy: 0.7323529411764707\n",
      "0:21:30 Batches 220, Train Loss: 2.748, Performance metric: 0.783, Mean Accuracy: 0.724264705882353\n",
      "0:21:40 Batches 230, Train Loss: 2.750, Performance metric: 0.784, Mean Accuracy: 0.7308823529411765\n",
      "0:21:49 Batches 240, Train Loss: 2.746, Performance metric: 0.783, Mean Accuracy: 0.7242647058823529\n",
      "0:21:58 Batches 250, Train Loss: 2.747, Performance metric: 0.784, Mean Accuracy: 0.7250000000000001\n",
      "0:22:06 Batches 260, Train Loss: 2.749, Performance metric: 0.786, Mean Accuracy: 0.7191176470588235\n",
      "0:22:26 Batches 270, Train Loss: 2.758, Performance metric: 0.788, Mean Accuracy: 0.7316176470588236\n",
      "0:22:39 Batches 280, Train Loss: 2.756, Performance metric: 0.790, Mean Accuracy: 0.736764705882353\n",
      "0:22:49 Batches 290, Train Loss: 2.756, Performance metric: 0.791, Mean Accuracy: 0.7419117647058824\n",
      "0:22:58 Batches 300, Train Loss: 2.756, Performance metric: 0.788, Mean Accuracy: 0.725735294117647\n",
      "VALIDATION: Performance metric: 0.772, Mean Accuracy: 0.7354900095147479\n",
      "0:23:32 Batches 310, Train Loss: 2.751, Performance metric: 0.788, Mean Accuracy: 0.7389705882352942\n",
      "0:23:41 Batches 320, Train Loss: 2.750, Performance metric: 0.789, Mean Accuracy: 0.7433823529411765\n",
      "0:23:50 Batches 330, Train Loss: 2.746, Performance metric: 0.787, Mean Accuracy: 0.7397058823529412\n",
      "0:23:58 Batches 340, Train Loss: 2.752, Performance metric: 0.784, Mean Accuracy: 0.7330882352941177\n",
      "0:24:06 Batches 350, Train Loss: 2.757, Performance metric: 0.790, Mean Accuracy: 0.725\n",
      "0:24:22 Batches 360, Train Loss: 2.759, Performance metric: 0.789, Mean Accuracy: 0.7426470588235294\n",
      "0:24:33 Batches 370, Train Loss: 2.753, Performance metric: 0.786, Mean Accuracy: 0.7411764705882353\n",
      "0:24:43 Batches 380, Train Loss: 2.756, Performance metric: 0.792, Mean Accuracy: 0.7360294117647059\n",
      "0:24:52 Batches 390, Train Loss: 2.762, Performance metric: 0.796, Mean Accuracy: 0.7426470588235294\n",
      "0:25:00 Batches 400, Train Loss: 2.758, Performance metric: 0.794, Mean Accuracy: 0.7205882352941176\n",
      "VALIDATION: Performance metric: 0.775, Mean Accuracy: 0.7165794481446242\n",
      "0:25:48 Batches 410, Train Loss: 2.762, Performance metric: 0.792, Mean Accuracy: 0.7382352941176471\n",
      "0:25:56 Batches 420, Train Loss: 2.753, Performance metric: 0.796, Mean Accuracy: 0.7455882352941177\n",
      "0:26:07 Batches 10, Train Loss: 2.625, Performance metric: 0.795, Mean Accuracy: 0.7235294117647059\n",
      "0:26:15 Batches 20, Train Loss: 2.719, Performance metric: 0.792, Mean Accuracy: 0.7198529411764706\n",
      "0:26:23 Batches 30, Train Loss: 2.721, Performance metric: 0.791, Mean Accuracy: 0.7272058823529413\n",
      "0:26:31 Batches 40, Train Loss: 2.755, Performance metric: 0.789, Mean Accuracy: 0.7191176470588235\n",
      "0:26:39 Batches 50, Train Loss: 2.760, Performance metric: 0.790, Mean Accuracy: 0.7352941176470589\n",
      "0:26:47 Batches 60, Train Loss: 2.771, Performance metric: 0.791, Mean Accuracy: 0.7176470588235294\n",
      "0:26:56 Batches 70, Train Loss: 2.729, Performance metric: 0.787, Mean Accuracy: 0.7169117647058824\n",
      "0:27:19 Batches 80, Train Loss: 2.721, Performance metric: 0.788, Mean Accuracy: 0.7360294117647059\n",
      "0:27:30 Batches 90, Train Loss: 2.717, Performance metric: 0.788, Mean Accuracy: 0.7323529411764707\n",
      "0:27:40 Batches 100, Train Loss: 2.714, Performance metric: 0.790, Mean Accuracy: 0.7330882352941177\n",
      "VALIDATION: Performance metric: 0.774, Mean Accuracy: 0.7450047573739296\n",
      "0:28:11 Batches 110, Train Loss: 2.708, Performance metric: 0.786, Mean Accuracy: 0.7323529411764707\n",
      "0:28:19 Batches 120, Train Loss: 2.716, Performance metric: 0.787, Mean Accuracy: 0.7294117647058824\n",
      "0:28:27 Batches 130, Train Loss: 2.713, Performance metric: 0.791, Mean Accuracy: 0.7242647058823529\n",
      "0:28:35 Batches 140, Train Loss: 2.712, Performance metric: 0.787, Mean Accuracy: 0.7330882352941177\n",
      "0:28:42 Batches 150, Train Loss: 2.726, Performance metric: 0.792, Mean Accuracy: 0.7198529411764706\n",
      "0:28:50 Batches 160, Train Loss: 2.728, Performance metric: 0.796, Mean Accuracy: 0.7345588235294118\n",
      "0:28:58 Batches 170, Train Loss: 2.722, Performance metric: 0.792, Mean Accuracy: 0.7375\n",
      "0:29:06 Batches 180, Train Loss: 2.720, Performance metric: 0.788, Mean Accuracy: 0.7323529411764707\n",
      "0:29:14 Batches 190, Train Loss: 2.726, Performance metric: 0.793, Mean Accuracy: 0.7433823529411765\n",
      "0:29:37 Batches 200, Train Loss: 2.721, Performance metric: 0.788, Mean Accuracy: 0.7066176470588236\n",
      "VALIDATION: Performance metric: 0.775, Mean Accuracy: 0.7180066603235016\n",
      "0:30:15 Batches 210, Train Loss: 2.723, Performance metric: 0.790, Mean Accuracy: 0.7147058823529412\n",
      "0:30:23 Batches 220, Train Loss: 2.726, Performance metric: 0.786, Mean Accuracy: 0.7426470588235294\n",
      "0:30:31 Batches 230, Train Loss: 2.724, Performance metric: 0.790, Mean Accuracy: 0.7213235294117648\n",
      "0:30:39 Batches 240, Train Loss: 2.724, Performance metric: 0.793, Mean Accuracy: 0.7323529411764707\n",
      "0:30:47 Batches 250, Train Loss: 2.722, Performance metric: 0.789, Mean Accuracy: 0.7382352941176471\n",
      "0:30:55 Batches 260, Train Loss: 2.726, Performance metric: 0.792, Mean Accuracy: 0.7227941176470589\n",
      "0:31:03 Batches 270, Train Loss: 2.726, Performance metric: 0.791, Mean Accuracy: 0.7294117647058824\n",
      "0:31:24 Batches 280, Train Loss: 2.724, Performance metric: 0.790, Mean Accuracy: 0.7205882352941176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:31:35 Batches 290, Train Loss: 2.723, Performance metric: 0.792, Mean Accuracy: 0.7426470588235294\n",
      "0:31:45 Batches 300, Train Loss: 2.722, Performance metric: 0.792, Mean Accuracy: 0.7360294117647059\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-6e3dd79b0be8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0my_true_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_model_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[0mperformance_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimal_thresholds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_accuracy_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_model_performance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-40e678ffed37>\u001b[0m in \u001b[0;36mcalc_model_predictions\u001b[1;34m(model, data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalc_model_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistortion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprint_model_performance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\teachinml\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1165\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32md:\\anaconda3\\envs\\teachinml\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    288\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\teachinml\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    521\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\teachinml\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    521\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_performance_metric = -1\n",
    "model_save_path = os.path.join('.', 'models', str(int(time.time())))\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "best_model_path = os.path.join(model_save_path, 'best_keras_model.h5py')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    count = 0\n",
    "  \n",
    "    performance_metrics = []\n",
    "    losses = []\n",
    "\n",
    "    for batch_inputs, batch_labels, _ in batch_generator(X_train, distortion=True):\n",
    "        loss = model.train_on_batch(batch_inputs, batch_labels)\n",
    "        losses.append(loss)\n",
    "\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            y_true_train, y_pred_train = calc_model_predictions(model, X_train.sample(frac=0.02, random_state=2))\n",
    "            performance_train, optimal_thresholds, mean_accuracy_train = calc_model_performance(y_true_train, y_pred_train)\n",
    "            \n",
    "            runtime = str(datetime.timedelta(seconds=int(time.time()-start_time)))\n",
    "            print('{} Batches {}, Train Loss: {:.3f}, Performance metric: {:.3f}, Mean Accuracy: {}' \\\n",
    "                  .format(runtime, count, np.mean(losses), performance_train, mean_accuracy_train))\n",
    "            \n",
    "        if count % 100 == 0:\n",
    "            y_true_val, y_pred_val = calc_model_predictions(model, X_validation)\n",
    "            performance_val, optimal_thresholds, mean_accuracy_val = calc_model_performance(y_true_val, y_pred_val)\n",
    "            \n",
    "            print('VALIDATION: Performance metric: {:.3f}, Mean Accuracy: {}'.format(performance_val, mean_accuracy_val))\n",
    "            \n",
    "            interim_model_path = os.path.join(model_save_path, \\\n",
    "                                              'batch-{}-performance-{}'.format(count, int(performance_val*1000)))\n",
    "            \n",
    "        \n",
    "            if performance_val > best_performance_metric:\n",
    "                best_performance_metric = performance_val\n",
    "                model.save(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model = load_model(best_model_path, custom_objects={'bp_mll_loss': bp_mll_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance: 0.775249709457181, Mean Accuracy: 0.7165794481446242\n"
     ]
    }
   ],
   "source": [
    "print_model_performance(loaded_model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.066745654,\n",
       " 0.13608873,\n",
       " 0.11436961,\n",
       " 0.099687755,\n",
       " 0.44896445,\n",
       " 0.22705011,\n",
       " 0.10063748,\n",
       " 0.27127913]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_opt_th(model, data):\n",
    "    y_true, y_pred = calc_model_predictions(model, data)\n",
    "    return calc_optimal_thresholds(y_true, y_pred)\n",
    "\n",
    "optimal_thresholds = calc_opt_th(loaded_model, X_validation)\n",
    "optimal_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_predicted_labels(label_binarizer, y_pred_values, thresholds):\n",
    "    y_predicted = (y_pred_values > thresholds).astype(int)\n",
    "    return label_binarizer.inverse_transform(np.array([y_predicted]))[0]\n",
    "\n",
    "def test_predicted_labels():\n",
    "    lb = MultiLabelBinarizer()\n",
    "    labels1 = ['class1', 'class2']\n",
    "    labels2 = ['class1']\n",
    "    lb.fit([labels1, labels2])\n",
    "    predicted_labels = calc_predicted_labels(lb, np.array([0.2, 0.5]), np.array([0.1, 0.6]))\n",
    "    assert len(predicted_labels) == len(labels2) == 1\n",
    "    assert predicted_labels[0] == labels2[0]\n",
    "    \n",
    "test_predicted_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def plot_img(img_file, teached_target, labels, y_pred_values, predicted_values):\n",
    "    plt.figure(figsize=(6,8))\n",
    "    img=mpimg.imread(os.path.join(posters_dir, img_file))\n",
    "\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    plt.imshow(img)\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    plt.barh(range(len(labels)), list(y_pred_values))\n",
    "    plt.yticks(range(len(labels)), list(labels), fontsize=12)\n",
    "    ax.set_xlim(right=1.0)\n",
    "\n",
    "    plt.gcf().text(0, 1.05, 'Labeled as {}'.format(teached_target), fontsize=18)\n",
    "    plt.gcf().text(0, 1, 'Predicted {}'.format(predicted_values), fontsize=18)\n",
    "    plt.gcf().text(0, 0.95, os.path.basename(img_file), fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_classifications_path = os.path.join('.', 'classification_examples')\n",
    "\n",
    "if os.path.exists(example_classifications_path):\n",
    "    shutil.rmtree(example_classifications_path)\n",
    "\n",
    "if not os.path.exists(example_classifications_path):\n",
    "    os.makedirs(example_classifications_path)\n",
    "\n",
    "count = 0\n",
    "for batch_inputs, batch_labels, img_files  in batch_generator(X_test, distortion=False):\n",
    "    predictions = loaded_model.predict(batch_inputs)\n",
    "    for i in range(batch_inputs.shape[0]):\n",
    "        count += 1\n",
    "        encoded_label = np.expand_dims(batch_labels[i], axis=0)\n",
    "        teached_labels = label_binarizer.inverse_transform(encoded_label)[0]\n",
    "        prediction = predictions[i]\n",
    "        predicted_labels = calc_predicted_labels(label_binarizer, prediction, optimal_thresholds)\n",
    "        plt = plot_img(img_files[i], teached_labels, label_binarizer.classes_, prediction, predicted_labels)\n",
    "        plt.savefig(os.path.join(example_classifications_path, img_files[i]), bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
