{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "import os\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import hamming_loss\n",
    "import shutil\n",
    "\n",
    "posters_dir = os.path.join('.', 'posters')\n",
    "model_save_path = os.path.join('.', 'best_keras_model.h5py')\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26585, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv('extended_movie_data_with_local_files.csv', sep=';')\n",
    "movies.head()\n",
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Workspaces\\movie-genre-ml\\helpers.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  filtered_movies['release_year'] = pd.to_numeric(filtered_movies.release_year)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10874, 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import helpers\n",
    "importlib.reload(helpers)\n",
    "\n",
    "movies = helpers.filter_movies(movies)\n",
    "movies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Comedy', 'Horror'], ['Drama', 'Sci-Fi']]\n",
      "[[0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres = movies.genres.tolist()\n",
    "genres = list(map(lambda x: x.split('|'), genres))\n",
    "label_binarizer = MultiLabelBinarizer()\n",
    "label_binarizer.fit(genres)\n",
    "print(genres[:2])\n",
    "print(label_binarizer.transform(genres[:2]))\n",
    "len(label_binarizer.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>tmdbId</th>\n",
       "      <th>poster_img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>131</td>\n",
       "      <td>1629</td>\n",
       "      <td>MatchMaker, The (1997)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>119632</td>\n",
       "      <td>20457</td>\n",
       "      <td>1629.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>870</td>\n",
       "      <td>30808</td>\n",
       "      <td>It Happens Every Spring (1949)</td>\n",
       "      <td>Comedy|Sci-Fi</td>\n",
       "      <td>41514</td>\n",
       "      <td>88288</td>\n",
       "      <td>30808.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>1692</td>\n",
       "      <td>91697</td>\n",
       "      <td>Pitfall (1948)</td>\n",
       "      <td>Film-Noir</td>\n",
       "      <td>40695</td>\n",
       "      <td>25688</td>\n",
       "      <td>91697.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>1984</td>\n",
       "      <td>104625</td>\n",
       "      <td>Apartment for Peggy (1948)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>40104</td>\n",
       "      <td>218212</td>\n",
       "      <td>104625.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>1286</td>\n",
       "      <td>71573</td>\n",
       "      <td>Whiteout (2009)</td>\n",
       "      <td>Action|Crime|Drama|Mystery|Thriller</td>\n",
       "      <td>365929</td>\n",
       "      <td>22787</td>\n",
       "      <td>71573.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  movieId                           title  \\\n",
       "131          131     1629          MatchMaker, The (1997)   \n",
       "870          870    30808  It Happens Every Spring (1949)   \n",
       "1692        1692    91697                  Pitfall (1948)   \n",
       "1984        1984   104625      Apartment for Peggy (1948)   \n",
       "1286        1286    71573                 Whiteout (2009)   \n",
       "\n",
       "                                   genres  imdbId  tmdbId  poster_img  \n",
       "131                        Comedy|Romance  119632   20457    1629.jpg  \n",
       "870                         Comedy|Sci-Fi   41514   88288   30808.jpg  \n",
       "1692                            Film-Noir   40695   25688   91697.jpg  \n",
       "1984                                Drama   40104  218212  104625.jpg  \n",
       "1286  Action|Crime|Drama|Mystery|Thriller  365929   22787   71573.jpg  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_validation = train_test_split(movies, test_size=0.1, random_state=1)\n",
    "X_train, X_test = train_test_split(X_train, test_size=0.1, random_state=1)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre count average 1.9856\n",
      "Genre count variance 1.07499264\n"
     ]
    }
   ],
   "source": [
    "length_of_genres = list(map(lambda x: len(x), genres))\n",
    "genres_count_average = np.mean(length_of_genres)\n",
    "print('Genre count average {}'.format(genres_count_average))\n",
    "genres_count_variance = np.var(length_of_genres)\n",
    "print('Genre count variance {}'.format(genres_count_variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10518518518518519"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres_test = X_test.genres.tolist()\n",
    "genres_test = list(map(lambda x: x.split('|'), genres_test))\n",
    "labels_test = label_binarizer.transform(genres_test)\n",
    "hamming_loss(labels_test, np.zeros(labels_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingAverage:\n",
    "    def __init__(self):\n",
    "        self._values = []\n",
    "    \n",
    "    def put(self, value):\n",
    "        self._values.append(value)\n",
    "        \n",
    "    def average(self):\n",
    "        return np.mean(self._values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "img_width, img_height = 224, 224\n",
    "#img_width, img_height = 299, 299\n",
    "\n",
    "def batch_generator(dataframe, distortion=False):\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         horizontal_flip=False\n",
    "    )   \n",
    "\n",
    "    row_generator = dataframe.sample(frac=1).iterrows()\n",
    "    \n",
    "    for batch_count in range(int(dataframe.shape[0]/batch_size)):\n",
    "        \n",
    "        batch_img = []\n",
    "        batch_labels = []\n",
    "        batch_img_files = []\n",
    "        \n",
    "        for img_count in range(batch_size):\n",
    "            _, row = next(row_generator)\n",
    "\n",
    "            img_path = os.path.join(posters_dir, row['poster_img'])\n",
    "            img = load_img(img_path, target_size=(img_width, img_height))\n",
    "            img = img_to_array(img)\n",
    "            if hasattr(img, 'close'):\n",
    "                img.close()\n",
    "            if distortion:\n",
    "                img = train_datagen.random_transform(img)\n",
    "            img = train_datagen.standardize(img)\n",
    "\n",
    "            label_list = row['genres'].split('|')\n",
    "            label = label_binarizer.transform([label_list])[0]\n",
    "\n",
    "            batch_img.append(img)\n",
    "            batch_labels.append(label)\n",
    "            batch_img_files.append(row['poster_img'])\n",
    "\n",
    "        yield [np.array(batch_img), np.array(batch_labels), batch_img_files]\n",
    "            \n",
    "    return\n",
    "\n",
    "#print(X_train.shape)\n",
    "#next(batch_generator(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_5:0\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model, Input\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.layers import concatenate\n",
    "\n",
    "base_model = applications.VGG16(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))\n",
    "#base_model = applications.InceptionV3(include_top=False, weights='imagenet', input_shape=(img_width, img_height, 3))\n",
    "\n",
    "print(base_model.input.name)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "\n",
    "#fg_distribution = Input(shape=(len(label_binarizer.classes_),), name='fg_distribution')\n",
    "#densities = Input(shape=(1,) , name='densities')\n",
    "#x = concatenate([x, fg_distribution, densities])\n",
    "#x = Flatten()(x)\n",
    "\n",
    "predictions = Dense(len(label_binarizer.classes_), activation='sigmoid')(x)\n",
    "model = Model(inputs=[base_model.input], outputs=predictions)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.525"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_hamming_loss(labels, batch):\n",
    "    predictions_sigmoid = model.predict_on_batch(batch_inputs)\n",
    "    predictions = np.where(predictions_sigmoid > 0.5, 1, 0)\n",
    "    return hamming_loss(batch_labels, predictions)\n",
    "\n",
    "x, labels, _ = next(batch_generator(X_train))\n",
    "calc_hamming_loss(labels, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches 10, Train Loss: 4.741, Hamming Loss: 0.124\n",
      "Batches 20, Train Loss: 4.955, Hamming Loss: 0.125\n",
      "Batches 30, Train Loss: 4.846, Hamming Loss: 0.128\n",
      "Batches 40, Train Loss: 4.836, Hamming Loss: 0.131\n",
      "Batches 50, Train Loss: 4.848, Hamming Loss: 0.130\n",
      "Batches 60, Train Loss: 4.886, Hamming Loss: 0.130\n",
      "Batches 70, Train Loss: 4.933, Hamming Loss: 0.131\n",
      "Batches 80, Train Loss: 4.903, Hamming Loss: 0.130\n",
      "Batches 90, Train Loss: 4.890, Hamming Loss: 0.130\n",
      "Batches 100, Train Loss: 4.849, Hamming Loss: 0.127\n",
      "Epoch: 0, Validation Loss: 0.094\n",
      "Batches 10, Train Loss: 4.742, Hamming Loss: 0.123\n",
      "Batches 20, Train Loss: 4.781, Hamming Loss: 0.125\n",
      "Batches 30, Train Loss: 4.784, Hamming Loss: 0.123\n",
      "Batches 40, Train Loss: 4.815, Hamming Loss: 0.123\n",
      "Batches 50, Train Loss: 4.739, Hamming Loss: 0.122\n",
      "Batches 60, Train Loss: 4.734, Hamming Loss: 0.121\n",
      "Batches 70, Train Loss: 4.763, Hamming Loss: 0.121\n",
      "Batches 80, Train Loss: 4.711, Hamming Loss: 0.120\n",
      "Batches 90, Train Loss: 4.690, Hamming Loss: 0.118\n",
      "Batches 100, Train Loss: 4.710, Hamming Loss: 0.117\n",
      "Epoch: 1, Validation Loss: 0.092\n",
      "Batches 10, Train Loss: 4.998, Hamming Loss: 0.121\n",
      "Batches 20, Train Loss: 4.668, Hamming Loss: 0.114\n",
      "Batches 30, Train Loss: 4.651, Hamming Loss: 0.111\n",
      "Batches 40, Train Loss: 4.637, Hamming Loss: 0.110\n",
      "Batches 50, Train Loss: 4.586, Hamming Loss: 0.108\n",
      "Batches 60, Train Loss: 4.546, Hamming Loss: 0.107\n",
      "Batches 70, Train Loss: 4.543, Hamming Loss: 0.106\n",
      "Batches 80, Train Loss: 4.582, Hamming Loss: 0.107\n",
      "Batches 90, Train Loss: 4.600, Hamming Loss: 0.107\n",
      "Batches 100, Train Loss: 4.618, Hamming Loss: 0.107\n",
      "Epoch: 2, Validation Loss: 0.083\n",
      "Batches 10, Train Loss: 4.739, Hamming Loss: 0.109\n",
      "Batches 20, Train Loss: 4.532, Hamming Loss: 0.104\n",
      "Batches 30, Train Loss: 4.616, Hamming Loss: 0.107\n",
      "Batches 40, Train Loss: 4.630, Hamming Loss: 0.107\n",
      "Batches 50, Train Loss: 4.546, Hamming Loss: 0.105\n",
      "Batches 60, Train Loss: 4.503, Hamming Loss: 0.104\n",
      "Batches 70, Train Loss: 4.485, Hamming Loss: 0.105\n",
      "Batches 80, Train Loss: 4.548, Hamming Loss: 0.105\n",
      "Batches 90, Train Loss: 4.524, Hamming Loss: 0.105\n",
      "Batches 100, Train Loss: 4.518, Hamming Loss: 0.104\n",
      "Epoch: 3, Validation Loss: 0.133\n",
      "Batches 10, Train Loss: 4.195, Hamming Loss: 0.094\n",
      "Batches 20, Train Loss: 4.315, Hamming Loss: 0.099\n",
      "Batches 30, Train Loss: 4.306, Hamming Loss: 0.098\n",
      "Batches 40, Train Loss: 4.271, Hamming Loss: 0.098\n",
      "Batches 50, Train Loss: 4.368, Hamming Loss: 0.100\n",
      "Batches 60, Train Loss: 4.446, Hamming Loss: 0.102\n",
      "Batches 70, Train Loss: 4.467, Hamming Loss: 0.101\n",
      "Batches 80, Train Loss: 4.503, Hamming Loss: 0.102\n",
      "Batches 90, Train Loss: 4.481, Hamming Loss: 0.102\n",
      "Batches 100, Train Loss: 4.495, Hamming Loss: 0.102\n",
      "Epoch: 4, Validation Loss: 0.097\n"
     ]
    }
   ],
   "source": [
    "best_hammond_loss = -1\n",
    "for epoch in range(5):\n",
    "    \n",
    "    count = 0\n",
    "  \n",
    "    hamming_loss_rolling = RollingAverage()\n",
    "    loss_rolling = RollingAverage()\n",
    "\n",
    "    for batch_inputs, batch_labels, _ in batch_generator(X_train, distortion=True):\n",
    "        hamming = calc_hamming_loss(batch_labels, batch_inputs)\n",
    "        hamming_loss_rolling.put(hamming)\n",
    "        loss = model.train_on_batch(batch_inputs, batch_labels)\n",
    "        loss_rolling.put(loss)\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print('Batches {}, Train Loss: {:.3f}, Hamming Loss: {:.3f}' \\\n",
    "                  .format(count, loss_rolling.average(), hamming_loss_rolling.average()))\n",
    "    \n",
    "    test_hamming_loss_rolling = RollingAverage()\n",
    "    for test_batch_inputs, test_batch_labels, _ in batch_generator(X_validation, distortion=False):\n",
    "        test_hamming_loss_rolling.put(calc_hamming_loss(test_batch_labels, test_batch_inputs))\n",
    "        \n",
    "    if test_hamming_loss_rolling.average() > best_hammond_loss:\n",
    "        best_hammond_loss = test_hamming_loss_rolling.average()\n",
    "        model.save(model_save_path)\n",
    "\n",
    "    print('Epoch: {}, Hamming Loss: {:.3f}'\n",
    "          .format(epoch, test_hamming_loss_rolling.average()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model = load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10151515151515152"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamming = RollingAverage()\n",
    "for batch_inputs, batch_labels, img_files in batch_generator(X_test, distortion=False):\n",
    "    hamming.put(calc_hamming_loss(batch_labels, batch_inputs))\n",
    "    \n",
    "hamming.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def plot_img(img_file, teached_target, labels, result):\n",
    "    plt.figure(figsize=(6,8))\n",
    "    img=mpimg.imread(os.path.join(posters_dir, img_file))\n",
    "\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    plt.imshow(img)\n",
    "\n",
    "\n",
    "    ax = plt.subplot(2, 1, 2)\n",
    "    plt.barh(range(len(labels)), list(result))\n",
    "    plt.yticks(range(len(labels)), list(labels), fontsize=12)\n",
    "    ax.set_xlim(right=1.0)\n",
    "\n",
    "    plt.gcf().text(0, 1.05, 'Teached as {}'.format(teached_target), fontsize=18)\n",
    "    plt.gcf().text(0, 1, os.path.basename(img_file), fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_classifications_path = os.path.join('.', 'classification_examples')\n",
    "\n",
    "if os.path.exists(example_classifications_path):\n",
    "    shutil.rmtree(example_classifications_path)\n",
    "\n",
    "if not os.path.exists(example_classifications_path):\n",
    "    os.makedirs(example_classifications_path)\n",
    "\n",
    "count = 0\n",
    "for batch_inputs, batch_labels, img_files  in batch_generator(X_test, distortion=False):\n",
    "    predictions = loaded_model.predict(batch_inputs)\n",
    "    for i in range(batch_inputs.shape[0]):\n",
    "        count += 1\n",
    "        encoded_label = np.expand_dims(batch_labels[i], axis=0)\n",
    "        teached_target = label_binarizer.inverse_transform(encoded_label)[0]\n",
    "        prediction = predictions[i]\n",
    "        plt = plot_img(img_files[i], teached_target, label_binarizer.classes_, prediction)\n",
    "        plt.savefig(os.path.join(example_classifications_path, img_files[i]), bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
